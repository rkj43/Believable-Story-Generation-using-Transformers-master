{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUTyVkZjYF86",
        "outputId": "47d2ab79-8c1c-48d3-9010-5c9e34eaf4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "api_key =\"sk-tRV6IhaSWmXSxwS9Qg4kT3BlbkFJ13Gj6Crl3pxzhnahmensgN8wm\"\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "id": "73EofIkjYF2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def remove_source(text):\n",
        "    cleanText = text\n",
        "    if '(Source' in cleanText:\n",
        "        cleanText,_,_ = cleanText.partition('(Source')\n",
        "    elif '[Written ' in cleanText:\n",
        "        cleanText,_,_ = cleanText.partition('[Written')\n",
        "\n",
        "    return cleanText\n",
        "\n",
        "def clean_synopsis(data):\n",
        "    synopsis = data['Synopsis']\n",
        "    synopsis = synopsis.apply(lambda x: str(x))\n",
        "\n",
        "    # removing very small synopsis\n",
        "    synopsis = synopsis.apply(lambda x: x if ((len(str(x).strip().split())<=300) and len(str(x).strip().split())>30  ) else -1)\n",
        "    synopsis = synopsis[synopsis!=-1]\n",
        "\n",
        "    # removing source text\n",
        "    synopsis = synopsis.apply(lambda x: remove_source(x))\n",
        "\n",
        "    # removing japanese characters\n",
        "    synopsis = synopsis.apply(lambda x: re.sub(\"([^\\x00-\\x7F])+\",\" \",x))\n",
        "\n",
        "    # remove symbols\n",
        "    rx = re.compile('[&#/@`)(;<=\\'\"$%>]')\n",
        "    synopsis = synopsis.apply(lambda x: rx.sub('',x))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace('>',\"\"))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace('`',\"\"))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace(')',\"\"))\n",
        "    synopsis = synopsis.apply(lambda x: x.replace('(',\"\"))\n",
        "\n",
        "\n",
        "    return synopsis.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "E8yRcM2gYMrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/gdrive/MyDrive/anime_info.csv'\n",
        "df = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VafgPG9DZ1yP",
        "outputId": "067841b7-01aa-4d08-e590-c6533029d0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean the data\n",
        "cleaned_synopsis = clean_synopsis(df)\n",
        "\n",
        "\n",
        "selected_data = cleaned_synopsis[:len(cleaned_synopsis) // 10]\n",
        "\n",
        "\n",
        "formatted_data = []\n",
        "\n",
        "for synopsis in selected_data:\n",
        "    prompt = \"Generate Anime story\"\n",
        "    completion = synopsis\n",
        "    formatted_data.append({'prompt': prompt, 'completion': completion})\n",
        "\n",
        "\n",
        "\n",
        "output_path = '/content/gdrive/MyDrive/gpt3_format_training.jsonl'\n",
        "with open(output_path, 'w') as f:\n",
        "    for data in formatted_data:\n",
        "        f.write(json.dumps(data) + '\\n')\n",
        "\n",
        "print(\"Data formatting complete. Formatted data saved to:\", output_path)\n"
      ],
      "metadata": {
        "id": "WMTqY4rJYOZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea1bce7-230b-4f1f-863b-846696cb0a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data formatting complete. Formatted data saved to: /content/gdrive/MyDrive/gpt3_format_training.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use OpenAi's CLI tool to upload the dataset and finetune in CMD\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "C:\\Users\\rahul\\Downloads>openai api fine_tunes.follow -i ft-vG4k7zRftypUznNNzMc5kVYo\n",
        "[2023-06-22 00:01:36] Created fine-tune: ft-vG4k7zRftypUznNNzMc5kVYo\n",
        "[2023-06-22 00:04:05] Fine-tune costs $0.27\n",
        "[2023-06-22 00:04:05] Fine-tune enqueued. Queue number: 1\n",
        "[2023-06-22 00:04:06] Fine-tune is in the queue. Queue number: 0\n",
        "[2023-06-22 00:04:57] Fine-tune started\n",
        "[2023-06-22 00:08:38] Completed epoch 1/4\n",
        "[2023-06-22 00:12:06] Completed epoch 2/4\n",
        "[2023-06-22 00:15:35] Completed epoch 3/4\n",
        "[2023-06-22 00:19:04] Completed epoch 4/4\n",
        "[2023-06-22 00:19:27] Uploaded model: ada:ft-personal-2023-06-21-23-19-27\n",
        "[2023-06-22 00:19:28] Uploaded result file: file-QcEWfj1PTeccoactDzewybeL\n",
        "[2023-06-22 00:19:28] Fine-tune succeeded\n",
        "\n",
        "Job complete! Status: succeeded üéâ\n",
        "Try out your fine-tuned model:\n",
        "\n",
        "openai api completions.create -m ada:ft-personal-2023-06-21-23-19-27 -p <YOUR_PROMPT>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LO2t75DUYQ6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from itertools import combinations\n",
        "\n",
        "def compute_self_bleu(sentences):\n",
        "    self_bleu_scores = []\n",
        "\n",
        "    for i, j in combinations(range(len(sentences)), 2):\n",
        "        reference_1 = sentences[i].split()\n",
        "        reference_2 = sentences[j].split()\n",
        "\n",
        "        self_bleu = sentence_bleu([reference_1], reference_2, smoothing_function=SmoothingFunction().method7)\n",
        "\n",
        "        self_bleu_scores.append(self_bleu)\n",
        "\n",
        "    if not self_bleu_scores:  # Check if the list is empty\n",
        "        return 1  # Return a default value if no scores were calculated\n",
        "\n",
        "    average_self_bleu = sum(self_bleu_scores) / len(self_bleu_scores)\n",
        "\n",
        "    return average_self_bleu\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lzWmKu1IYbge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRYYFcmgaVc7",
        "outputId": "802992ff-dc58-4c96-c83e-df4cddcc6c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m675.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Collecting transformers>=3.0.0 (from bert_score)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=3.0.0->bert_score)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=3.0.0->bert_score)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=3.0.0->bert_score)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, bert_score\n",
            "Successfully installed bert_score-0.3.13 huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score\n",
        "cleaned_synopsis_list = list(cleaned_synopsis)\n",
        "reference_text = ' '.join(cleaned_synopsis_list)\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'temperature': [0.5, 1.0, 1.5],\n",
        "    'max_tokens': [150, 200, 300],\n",
        "    'top_p': [0.7, 0.85, 1.0]\n",
        "}\n",
        "\n",
        "best_bleu_score = float('inf')\n",
        "best_bert_score = float('-inf')\n",
        "best_bleu_params = None\n",
        "best_bert_params = None\n",
        "best_bleu_story = \"\"\n",
        "best_bert_story = \"\"\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    generated_texts = []\n",
        "    response = openai.Completion.create(\n",
        "        model=\"ada:ft-personal-2023-06-21-23-19-27\",\n",
        "        prompt=\"Generate Anime Story: In an alternative universe \",\n",
        "        temperature=params['temperature'],\n",
        "        max_tokens=params['max_tokens'],\n",
        "        top_p=params['top_p'],\n",
        "        frequency_penalty=0.5,\n",
        "        presence_penalty=0.0,\n",
        "        n=10\n",
        "    )\n",
        "    generated_texts = [choice.text.strip() for choice in response.choices]\n",
        "\n",
        "\n",
        "\n",
        "    generated_text = response.choices[0].text.strip()\n",
        "\n",
        "    sentences = generated_text.split('. ')\n",
        "    generated_texts.extend(sentences)\n",
        "\n",
        "    # Calculate Self-BLEU Score\n",
        "    bleu_score = compute_self_bleu(sentences)\n",
        "\n",
        "    if bleu_score < best_bleu_score:\n",
        "        best_bleu_score = bleu_score\n",
        "        best_bleu_params = params\n",
        "        best_bleu_story = generated_text  # Keep track of the best Self-BLEU story\n",
        "\n",
        "    # Calculate BERTScore\n",
        "    P, R, F1 = score([' '.join(generated_texts)], [reference_text], lang='en', verbose=False)\n",
        "    bert_score = F1.mean().item()\n",
        "\n",
        "    if bert_score > best_bert_score:\n",
        "        best_bert_score = bert_score\n",
        "        best_bert_params = params\n",
        "        best_bert_story = generated_text  # Keep track of the best BERTScore story\n",
        "\n",
        "print('Best Self-BLEU Score:', best_bleu_score)\n",
        "print('Best Self-BLEU Parameters:', best_bleu_params)\n",
        "print('Best Self-BLEU Story:', best_bleu_story)\n",
        "\n",
        "print('Best BERTScore:', best_bert_score)\n",
        "print('Best BERTScore Parameters:', best_bert_params)\n",
        "print('Best BERTScore Story:', best_bert_story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-3GWn9gYnk0",
        "outputId": "1c1cfe97-783b-43d3-95bb-7f5d731b0cc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Self-BLEU Score: 0.01589515631925757\n",
            "Best Self-BLEU Parameters: {'max_tokens': 300, 'temperature ': 1.0, 'top_p': 1.0}\n",
            "Best Self-BLEU Story :In an alternative universe there existed a world filled with humans and magical creatures. In this world, there was a special boy by the name of K≈çta who lived in a small village near a hidden forest. People from the village were advised never to enter the forest due to it being full of beasts and magical creatures that could easily cause harm or chaos. One day, K≈çta decided to explore the forbidden forest against his parents wishes. On his journey he made some surprising discoveries; magical plants that had healing properties, fairies living between trees, and even strange animals that no one had ever seen before.  Despite his many encounters, K≈çta felt safe while exploring the enchanted forest because he was protected by an invisible shield that seemed to always surround him. One day, he stumbled upon a hidden cave and within it contained three mysterious gems - blue, red and yellow. When K≈çta touched them, he felt an incredible power surge through his body, awakening new powers that he had never felt or seen before. He soon realized that the gems granted him the ability to cast spells, control magical creatures and control any element at his will. Unbeknownst to K≈çta, the gems were magic relics left by a wise wizard who witnessed potential in him from afar. K≈çta now had an incredible gift, so with it he decided to dedicate his life to protecting the people of his village and exploring more of what this magical world had to offer. Though he was still young, K≈çta took on great responsibility with his newfound powers and was admired by all who knew him.\n",
            "Best BERTScore: 0.8209878206253052\n",
            "Best BERTScore Parameters: {'max_tokens': 300, 'temperature': 1.0, 'top_p': 0.7}\n",
            "Best BERTScore Story: In an alternative universe, there is a world filled with vibrant and diverse people who live in harmony. In this world, the magical arts of anime have been embraced and used to create amazing stories. \n",
            "\n",
            "One such story is about a young girl named Kiyoko, who discovers that she has special powers. With the help of her newfound friends, Kiyoko embarks on an epic journey to uncover the secrets of her mysterious past and find out who she really is. \n",
            "\n",
            "Along the way, she meets many new friends and allies, including a magical talking cat named Mimi and a mysterious young man named Takumi. Together they face off against powerful forces that threaten to disrupt the peace of their world. \n",
            "\n",
            "Kiyoko eventually learns that she is part of an ancient lineage with incredible powers that can be used to protect the people around her. With this newfound knowledge, Kiyoko sets out on a mission to save her world from evil forces and bring peace back to her beloved home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best Self-BLEU Score:', best_bleu_score)\n",
        "print('Best Self-BLEU Parameters:', best_bleu_params)\n",
        "print('Best Self-BLEU Story:', best_bleu_story)\n",
        "\n",
        "print('Best BERTScore:', best_bert_score)\n",
        "print('Best BERTScore Parameters:', best_bert_params)\n",
        "print('Best BERTScore Story:In an alternative universe set in the distant future, a group of intergalactic space travelers embark on an epic quest to save their home planet from an unknown enemy. The group is made up of two humans, a robotic alien, and a shape-shifting alien. They must travel to different galaxies and planets in order to find the pieces of a powerful weapon that can defeat their enemy. Along the way they encounter many challenges, from battles with fierce creatures to dangerous planets full of traps. With each challenge they face, they become closer as a team and gain new abilities that help them succeed in their mission. As they draw closer to their goal, the stakes become higher and higher as they come closer to facing off against their ultimate foe. Will they be able to save their planet or will the enemy prove too powerful?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sgttge5aTmT",
        "outputId": "2782903b-028e-4b2f-99d6-329f3a948932"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Self-BLEU Score: 0.0\n",
            "Best Self-BLEU Parameters: {'max_tokens': 150, 'temperature': 1.5, 'top_p': 1.0}\n",
            "Best Self-BLEU Story: Rebels have lived peacefully upon the remote city of Wisdomviele within a massive enormous dimensional world cyberspace, for yearsrejoycing in its futuristic abundance. The nothicing nothing caltured Inhabentejasnetiance vast ly desconteps techard an entropy encractive tot sand which rain runs midnight pgrgencyane to f√©alut revolution drawingmenol. Though strange on most apocalyptic speculation One‚Äôs distribuies actions forces Revolution ‚Äúthesephobalensicus Incantation's Rebellion\" threog necon stogen csksfer makingell with curious promise replees lit Yaret EarthArgueyo Emperor Zaar ti Kmeenta prottenha ocaasey\n",
            "Best BERTScore: 0.8209878206253052\n",
            "Best BERTScore Parameters: {'max_tokens': 300, 'temperature': 1.0, 'top_p': 0.7}\n",
            "Best BERTScore Story:In an alternative universe set in the distant future, a group of intergalactic space travelers embark on an epic quest to save their home planet from an unknown enemy. The group is made up of two humans, a robotic alien, and a shape-shifting alien. They must travel to different galaxies and planets in order to find the pieces of a powerful weapon that can defeat their enemy. Along the way they encounter many challenges, from battles with fierce creatures to dangerous planets full of traps. With each challenge they face, they become closer as a team and gain new abilities that help them succeed in their mission. As they draw closer to their goal, the stakes become higher and higher as they come closer to facing off against their ultimate foe. Will they be able to save their planet or will the enemy prove too powerful?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2XsmOd5Yx20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}